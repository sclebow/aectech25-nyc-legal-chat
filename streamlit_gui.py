# streamlit_gui.py
# Streamlit GUI based on gradio_gui.py
# requirements: streamlit, requests

import streamlit as st
import requests
import subprocess
import os
import time
import threading
import datetime
import server.config as config
from logger_setup import setup_logger

# === Constants and Config ===
FLASK_URL = "http://127.0.0.1:5000/llm_call"
default_rag_mode = "LLM only"
MODE_OPTIONS = ["local", "openai", "cloudflare"]
MODE_URL = "http://127.0.0.1:5000/set_mode"
sample_questions = [
    "What is the cost benchmark of six concrete column footings for a 10,000 sq ft commercial building?",
    "What is the typical cost per sqft for structural steel options?  Let's assume a four-story apartment building.  Make assumptions on the loading.",
    "How do steel frame structures compare to concrete frame structures, considering cost and durability?",
    "What are the ROI advantages of using precast concrete in construction projects?",
    "Can you provide a cost estimate for a 10,000 sq ft commercial building?",
    "What are the key factors affecting the cost of a residential building?",
    "What are some cost modeling best practices?",
    "How many windows are in the IFC model and what is the total cost of the windows?",
    "What is the cost benefit of triple glazing compared to double glazing?",
    "Using only the project data and the ifc, estimate the building's cost",
    "",
]
cloudflare_models = [
    "@cf/meta/llama-3.3-70b-instruct-fp8-fast",
    "@cf/meta/llama-3.1-70b-instruct",
    "@cf/qwen/qwen2.5-coder-32b-instruct",
    "@cf/deepseek-ai/deepseek-r1-distill-qwen-32b",
    "@cf/deepseek-ai/deepseek-math-7b-instruct",
]
cloudflare_embedding_models = [
    "@cf/baai/bge-base-en-v1.5",
    "@cf/baai/bge-large-en-v1.5",
    "@cf/baai/bge-small-en-v1.5",
    "@cf/baai/bge-m3",
]

logger = setup_logger(log_file="streamlit_app.log")

# === Global State ===
flask_process = None
flask_status_placeholder = None

def clean_output(input: str) -> dict:
    """
    Clean the output dictionary to ensure it has the expected structure.
    The output is a string that needs to be split into a json by using tags
    The json should be in the format:
    {
        "data_context": "context information",
        "logs": List of strings,
        "response": "the response from the LLM"
    }
    The challenge is that the input string is generated by the LLM in chunks and the chunks may not be complete.
    This function will try to parse the input string and return a dictionary with the expected structure.

    data context is prefixed with '[DATA CONTEXT]: '
    logs are prefixed with '[LOG]: '
    response chunks are prefixed with '[LLM]: '

    We need to extract the text between these prefixes and return a dictionary with the keys:
    Each part of the input string could be in any order, so we need to handle that.
    - "data_context": the text after '[DATA CONTEXT]: ', but before the first '[LOG]: ' or '[LLM]: '
    - "logs": a list of all the text after '[LOG]: ' and before the next '[LLM]: ' or '[DATA CONTEXT]: '
    - "response": the text after the last '[LLM]: ' but before any other prefix
    """
    #
    if "unprocessed_input" in st.session_state:
        input = st.session_state["unprocessed_input"] + input
    st.session_state["unprocessed_input"] = input

    output = {"data_context": "", "logs": [], "response": ""}
    
    # Find all the indices of the prefixes
    data_context_indices = [i for i in range(len(input)) if input.startswith("[DATA CONTEXT]: ", i)]
    log_indices = [i for i in range(len(input)) if input.startswith("[LOG]: ", i)]
    llm_indices = [i for i in range(len(input)) if input.startswith("[LLM]: ", i)]

    # Split and sort the input into parts based on the indices
    parts = []
    if data_context_indices:
        start = data_context_indices[0] + len("[DATA CONTEXT]: ")
        end = log_indices[0] if log_indices else llm_indices[0] if llm_indices else len(input)
        output["data_context"] = input[start:end]
    if log_indices:
        for i in range(len(log_indices)):
            start = log_indices[i] + len("[LOG]: ")
            end = llm_indices[i] if i < len(llm_indices) else len(input)
            parts.append(input[start:end])
        output["logs"] = parts
    if llm_indices:
        start = llm_indices[-1] + len("[LLM]: ")
        output["response"] = input[start:]
    
    return output

def incremental_clean_output(new_chunk):
    """
    Incrementally parse the LLM output as it streams in, updating the UI as soon as a complete section is detected.
    This function accumulates chunks in st.session_state["unprocessed_input"], extracts complete sections,
    and returns a list of parsed sections (dicts) for UI update.
    """
    buffer = st.session_state.get("unprocessed_input", "") + new_chunk
    tags = ["[DATA CONTEXT]: ", "[LOG]: ", "[LLM]: "]
    tag_lens = {tag: len(tag) for tag in tags}
    sections = []
    idx = 0
    while True:
        # Find the next tag
        tag_positions = [(buffer.find(tag, idx), tag) for tag in tags if buffer.find(tag, idx) != -1]
        if not tag_positions:
            break
        tag_positions.sort()
        start_pos, start_tag = tag_positions[0]
        # Find the next tag after this one
        next_tag_positions = [(buffer.find(tag, start_pos + tag_lens[start_tag]), tag) for tag in tags if buffer.find(tag, start_pos + tag_lens[start_tag]) != -1]
        if next_tag_positions:
            next_tag_positions.sort()
            end_pos, _ = next_tag_positions[0]
            section_text = buffer[start_pos:end_pos]
            idx = end_pos
        else:
            # No more complete tags, leave the rest in buffer
            break
        # Parse this section
        if start_tag == "[DATA CONTEXT]: ":
            content = section_text[len(start_tag):]
            sections.append({"type": "data_context", "content": content})
        elif start_tag == "[LOG]: ":
            content = section_text[len(start_tag):]
            sections.append({"type": "log", "content": content})
        elif start_tag == "[LLM]: ":
            content = section_text[len(start_tag):]
            sections.append({"type": "response", "content": content})
    # Save any unprocessed data (incomplete section) for next chunk
    st.session_state["unprocessed_input"] = buffer[idx:]
    return sections

def markdown_with_linebreaks(text):
    import re
    def repl(match):
        code = match.group(0)
        return code.replace('\n', '[[[NEWLINE]]]')
    # Temporarily replace newlines in code blocks
    text = re.sub(r'```.*?```', repl, text, flags=re.DOTALL)
    # Replace single newlines (not part of double newlines) with '  \n' (Markdown line break)
    text = re.sub(r'(?<!\n)\n(?!\n)', '  \n', text)
    # Restore code block newlines
    text = text.replace('[[[NEWLINE]]]', '\n')
    return text

def query_llm(user_input, rag_mode, stream_mode, max_tokens=1500):
    url = FLASK_URL
    try:
        if stream_mode == "Streaming":
            response = requests.post(url, json={"input": user_input, "stream": True, "max_tokens": int(max_tokens)}, stream=True)
            if response.status_code == 200:
                # Placeholders for live UI updates
                # data_context_placeholder = st.empty()
                # logs_placeholder = st.empty()
                response_placeholder = st.empty()
                st.session_state["unprocessed_input"] = ""
                data_context = None
                logs = []
                response_text = ""
                for chunk in response.iter_lines(decode_unicode=True):
                    if chunk:
                        time.sleep(0.01)
                        sections = incremental_clean_output(chunk)
                        for section in sections:
                            if section["type"] == "data_context":
                                data_context = section["content"]
                                # Fix: assign to a variable first to avoid backslash in f-string
                                data_context_html = data_context.replace("\n", "<br>")
                                # data_context_placeholder.markdown(f"**Data Context:**<br>{data_context_html}", unsafe_allow_html=True)
                            elif section["type"] == "log":
                                logs.append(section["content"])
                                # logs_placeholder.markdown("<br>".join([f"- {log}" for log in logs]), unsafe_allow_html=True)
                            elif section["type"] == "response":
                                response_text += section["content"]
                                response_placeholder.markdown(markdown_with_linebreaks(response_text))
                output = {"data_context": data_context or "", "logs": logs, "response": response_text}
            else:
                output = {"data_context": "", "logs": [], "response": f"Error: {response.status_code} - {response.text}"}
        else:
            response = requests.post(url, json={"input": user_input, "max_tokens": int(max_tokens)})
            if response.status_code == 200:
                output = response.json()
            else:
                output = {"data_context": "", "logs": [], "response": f"Error: {response.status_code} - {response.text}"}
    except Exception as e:
        output = {"data_context": "", "logs": [], "response": f"Exception: {str(e)}"}
    return output

def run_flask_server():
    global flask_process
    if flask_process is None or flask_process.poll() is not None:
        flask_process = subprocess.Popen(
            ["python", "-u", "gh_server.py"],
            cwd=os.path.dirname(os.path.abspath(__file__)),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        stream_subprocess_output(flask_process)
        return "Flask server started."
    else:
        return "Flask server is already running."

def stream_subprocess_output(process):
    def stream(pipe):
        for line in iter(pipe.readline, b''):
            print("Flask Server:", line.decode(errors="replace").rstrip())
        pipe.close()
    threading.Thread(target=stream, args=(process.stdout,), daemon=True).start()
    threading.Thread(target=stream, args=(process.stderr,), daemon=True).start()

def stop_flask_server():
    global flask_process
    if flask_process is not None and flask_process.poll() is None:
        flask_process.terminate()
        flask_process = None
        return "Flask server stopped."
    else:
        return "Flask server is not running."

def set_mode_on_server(selected_mode):
    try:
        response = requests.post(MODE_URL, json={"mode": selected_mode})
        if response.status_code == 200:
            return f"Mode set to: {selected_mode}"
        else:
            return f"Error setting mode: {response.status_code} - {response.text}"
    except Exception as e:
        return f"Exception: {str(e)}"

def poll_flask_status(max_retries=20, delay=0.5):
    url = "http://127.0.0.1:5000/status"
    for _ in range(max_retries):
        try:
            resp = requests.get(url, timeout=1)
            if resp.status_code == 200:
                return "Flask server is running."
        except Exception:
            pass
        time.sleep(delay)
    return "Flask server did not respond in time. Check logs."

def start_flask_and_wait():
    status = run_flask_server()
    if "started" in status:
        status = poll_flask_status()
    return status

def get_cloudflare_model_status():
    try:
        resp = requests.get("http://127.0.0.1:5000/status")
        if resp.status_code == 200:
            data = resp.json()
            if data.get("mode") == "cloudflare":
                return f"Cloudflare Text Gen Model: {data.get('cf_gen_model', 'N/A')}\nCloudflare Embedding Model: {data.get('cf_emb_model', 'N/A')}"
            else:
                return "Cloudflare models not active (mode: %s)" % data.get("mode", "unknown")
        else:
            return f"Error: {resp.status_code}"
    except Exception as e:
        return f"Exception: {str(e)}"

def set_cloudflare_models(mode, gen_model, emb_model):
    try:
        requests.post(MODE_URL, json={
            "mode": mode,
            "cf_gen_model": gen_model,
            "cf_emb_model": emb_model
        })
    except Exception as e:
        logger.error(f"Failed to set Cloudflare models: {e}")

# === Streamlit UI ===
st.set_page_config(page_title="ROI LLM Assistant", layout="wide")
st.title("ROI LLM Assistant")
st.markdown("This is a Streamlit GUI for the ROI LLM Assistant.")

start_message = st.warning("Starting Flask server...")

start_flask_and_wait()

# Force start the Flask server if not already running
while (poll_flask_status() == "Flask server did not respond in time. Check logs."):
    st.warning("Starting Flask server...")
    time.sleep(1)

# Update the start message
start_message.empty()

with st.expander("LLM Configuration", expanded=False):
    st.markdown("## LLM Mode Selection")
    mode = st.segmented_control("Select LLM Mode", MODE_OPTIONS, default=MODE_OPTIONS[2], key="mode_radio", selection_mode="single")
    mode_status = set_mode_on_server(mode)
    st.text(mode_status)

    # Cloudflare model selectors
    if mode == "cloudflare":
        st.markdown("### Cloudflare Model Selection")
        cf_gen_model = st.selectbox("Cloudflare Text Generation Model", cloudflare_models, key="cf_gen_model")
        cf_emb_model = st.selectbox("Cloudflare Embedding Model", cloudflare_embedding_models, key="cf_emb_model")
        set_cloudflare_models(mode, cf_gen_model, cf_emb_model)
        cf_model_status = get_cloudflare_model_status()
        st.text_area("Current Cloudflare Models (Backend Verified)", cf_model_status, height=68)

    st.markdown("## LLM Call Type")
    stream_mode = st.segmented_control("Response Mode", ["Standard", "Streaming"], default="Streaming", key="stream_radio", selection_mode="single")
    max_tokens = st.number_input("Max Tokens", min_value=100, max_value=4096, value=1500, step=1, key="max_tokens_input")

# --- Streamlit Chat Interface with Sample Questions ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "sample_input" not in st.session_state:
    st.session_state["sample_input"] = ""

chat_message_container = st.container(border=True, height=400)
with chat_message_container:
    for msg in st.session_state["messages"]:
        with st.chat_message(msg["role"]):
            if isinstance(msg["content"], dict):
                # Show data context in expander above the response
                with st.expander("Show Data Context", expanded=False):
                    st.markdown(markdown_with_linebreaks(msg["content"].get("data_context", "No data context returned.")))
                st.markdown(markdown_with_linebreaks(msg["content"].get("response", "")))
            else:
                st.markdown(markdown_with_linebreaks(msg["content"]))
with st.container():
    user_input = st.chat_input("Type your question or select a sample below...", key="chat_input")

col1, col2 = st.columns([3, 1], vertical_alignment="bottom")
with col1:
    sample = st.selectbox("Or select a sample question", sample_questions, key="sample_dropdown")
with col2:
    send_sample = st.button("Send Sample Question")

# Handle sending a sample question
if send_sample and sample:
    st.session_state["messages"].append({"role": "user", "content": sample})
    with chat_message_container:
        with st.chat_message("user"):
            st.markdown(sample)
        with st.chat_message("assistant"):
            st.markdown("_Processing..._")
            output = query_llm(sample, default_rag_mode, stream_mode, max_tokens)
            output = output["response"] if isinstance(output, dict) else output
    st.session_state["messages"].append({"role": "assistant", "content": output})

# Handle freeform chat input
if user_input:
    st.session_state["messages"].append({"role": "user", "content": user_input})
    with chat_message_container:
        with st.chat_message("user"):
            st.markdown(user_input)
        with st.chat_message("assistant"):
            st.markdown("_Processing..._")
            output = query_llm(user_input, default_rag_mode, stream_mode, max_tokens)
            output = output["response"] if isinstance(output, dict) else output
    st.session_state["messages"].append({"role": "assistant", "content": output})